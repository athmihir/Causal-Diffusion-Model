{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b637e27",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b264d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asardesai/Projects/Causal-Diffusion-Model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-23 19:27:28 - INFO - DEBUG\n"
     ]
    }
   ],
   "source": [
    "from barmnist import BarMNIST\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from cdm.utils import visualize_image\n",
    "import os\n",
    "from diffusers import DDPMScheduler\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from cdm.scm import EdgeType, Vertex, Edge, SCM\n",
    "from cdm.constants import IMAGE_CHANNELS, IMAGE_RESOLUTION\n",
    "from cdm.causal_image_model import CausalImageModel\n",
    "from cdm.utils import mnist_y_labels\n",
    "import time\n",
    "\n",
    "# Setup the logger\n",
    "load_dotenv()\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"celeba.log\"),  # Log to file\n",
    "        logging.StreamHandler()          # Log to console\n",
    "    ],\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger.setLevel(os.getenv('LOG_LEVEL'))\n",
    "logger.info(os.getenv('LOG_LEVEL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE'))\n",
    "NUM_EPOCHS = int(os.getenv('NUM_EPOCHS'))\n",
    "MODEL_PATH = 'mnist_model.pt'\n",
    "DEVICE_NAME = os.getenv('DEVICE')\n",
    "DEVICE = torch.device(DEVICE_NAME) if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ab36a",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a7cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing BAR MNIST\n",
      "Causal Diagram:frontdoor\n",
      "BAR MNISTfrontdoor dataset already exists\n"
     ]
    }
   ],
   "source": [
    "cg = 'frontdoor'\n",
    "\n",
    "BarMNIST(cg=cg, ow=True)\n",
    "trans_f = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                ])\n",
    "train_set = BarMNIST(cg=cg, root='./', env='train', transform=trans_f)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                            drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8415cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([9, 5, 9, 8, 9, 0, 0, 8]), tensor([1, 1, 1, 0, 0, 1, 0, 1]), tensor([0, 1, 0, 0, 1, 1, 1, 0])]\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDw7+0v+mX/AI9/9al/tH/pj/49/wDWqhXT3An0myf+y7SJYmjUPqP8Thh0XJ+XPPA5PJ4xgevU4pzODSVTV+UV+hyVeSm1FLV+dl/X4lFlv0ZFbTLlWdtqgowLNxwOOTyPzHrVU6iQSDDgj/a/+tWsnlx2V7qGr307X7AmzSOZSHL53EgHIHOewIz1zXNszOxZiWYnJJOSTSpcUZrO96u392P4aCoKNS91t62+Xf8AzErTTTEg06W7v3MTEMsEByJHYEDdjH3Qc89yCO1V9Ov2067FwtvbzkAgJcR715749RTL2+udQuWnuZWkc5xkkhQSTgegyTx714cnKU+RaI0lKcqnItEtX39CBjuYtgDJzgdBSUUVsdB//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAByElEQVR4AdWUPUiVURjHf78yopKCUAgC0bAl6cMoxNGwJcWtIaKvUWqJxKGlTXBvqpC2Ah2ChqilUYRycQiCQBqCIIjAGgKrc88t773nvef6hjr48PLyPP/znP/5n4/nUX+z2bZjswkrfNuHtG0rxG6f7f+n0g54CB9gFH7CGFyAbvgCJ+rekSVtRJd1Wr/rkN7Xy7pL9+lTvah3akTU3KZemHZYF/SzPtHd+qiQdzRFWpI+iOuv6gsdj6LS6XpWb+vxhoE8aZ8O6Jze1J0Nc2rBD/2qL2tA1QvvNGPnYBz6CqPhavfCCtyCVWiHd4WcdJW1+KOe15Nr8T8nnGBV+B59rkFswfJPahAmYBiC5PDNwkE4BFci+ArmYQkOFGQSj7QJXIXuwv7oXYNL8B4+QTiwqxF8DL+qeem/Ij3FNhznt5+jrq+cTE5p0nDjXZGjN8NUB5fe/iQcg+t1U/NuOaVnYACe5WkaR0ooDW3pVHztQ41TW0QV1hYWusk3vaE9LZLSoXztT8VyCs0ptIxc7adsf+PwlAt2BDqgP1ZLJ7wpJKwLpIvd07ca2t3r2EnT4VJx4fYX4TTMwFwsynVFNUuorNwM3xD2B5mCOkNkxYhuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample = None\n",
    "x_sample_batch = None\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    x_sample_batch = x\n",
    "    x_sample = x_sample_batch[0]\n",
    "    print(mnist_y_labels(y))\n",
    "    break\n",
    "\n",
    "visualize_image(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbf8e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCok4u4ZQg8u6ngUXE29HkkUA7kjUDH3EXJyQP9p1yJ5ILuawvmuZ4o70ssbq0y4Mi5ZFkUnPyq8ZHzEbigwNpAsNp8Mtw1xaYtWjkMTo0KRGMsxAcpHICpHCA9QrJgcnNa006CTT7i9uPPWWOaN5VmlRH2glV6rgBQSuCOpGQDtxm6seZ6u7s7+r27pL8HbbVk+63P3+q2V2tH8n2Xa25HqGp6ZYTsbzF4RKSqPDGkgkIVSzKyswztMpOQTkADBJG7puh6WLUiSHUpJd5LSxXKAPnlSdwHIXaDjjjgnqc1fKbWr5YbJJoldw0aRRIBh+GIwAVBBwGyCWRsEjdWHKthdN51jpNxcRPl8pJFEV3EsAy4OG2sp645GAowq1Rqyjy+yk4313S6Ls0lr97d/JEIxlHlm+ivZve7tflutnvte+xZLTXGnrDMypOzBot5jukLRJh3XguxDnOQzfKrgghTUN7Ncw+ZaRRus0ar5cFsgZMmTDITn97yY88EglVK5HF/SYVYFPl2W+ppEA0ayblXayglwSANxAAwADwAeaz/ABK72msQRlvORYGwJVHBdWyRgDBzITxjlV9BW2GXNa0Uvteujfyvaz30LhJTrezpq2tvxSf333067a32IxE1v9nknMFw0KyyRDasqfMFIGZD0jVsBQT8zY2fKprR3lzpY8pLqeHeFfylhkl2jaAPnTCtkAHOOhH0rXXR4RoaPJPcyK8phMXm7Iwqu7DCJtUZIXt2+ueb8RRpZzWTkz3DXNos7Ge7mYqSzDaDvzjjvk8nmsYU1Nyw1bZO/dXaVvi028v8xRqWjbRN/wB2Mv8A0q/X+rtn/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "img2 = x_sample.unsqueeze(0)\n",
    "noise = torch.randn_like(img2)\n",
    "noisy_image = noise_scheduler.add_noise(img2, noise, torch.tensor([80]))\n",
    "noisy_image = noisy_image[0]\n",
    "visualize_image(noisy_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144d4e9",
   "metadata": {},
   "source": [
    "### Initialize the Model\n",
    "\n",
    "The Diffusion model will represent the node I. Node I takes the generative features as inputs along with the noised version of the image.\n",
    "It will then attempt to predict the noise aka denoising the image to genearte the original input image through a series of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da210bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 19:27:45 - INFO - Initializing a new model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalImageModel(\n",
       "  (scm): SCM(\n",
       "    (layers): ModuleDict(\n",
       "      (Vertex(label:'Color, dimensions: '(2,)'')): SCMFunction(\n",
       "        (cnn1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cnn2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cnn3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (linear): Linear(in_features=49, out_features=2, bias=True)\n",
       "        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (Vertex(label:'Bar, dimensions: '(2,)'')): SCMFunction(\n",
       "        (cnn1): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cnn2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cnn3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (linear): Linear(in_features=49, out_features=2, bias=True)\n",
       "        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (Vertex(label:'Digit, dimensions: '(10,)'')): SCMFunction(\n",
       "        (cnn1): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cnn2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cnn3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (linear): Linear(in_features=49, out_features=10, bias=True)\n",
       "        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (unet): UNet2DConditionModel(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_proj): Timesteps()\n",
       "    (time_embedding): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): AttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Attention(\n",
       "            (group_norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): AttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Attention(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): AttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Attention(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): AttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Attention(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): AttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Attention(\n",
       "            (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): AttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Attention(\n",
       "            (group_norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2DCrossAttn(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=14, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=14, out_features=512, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_vertex = Vertex('Digit', (10,))\n",
    "b_vertex = Vertex('Bar', (2,))\n",
    "c_vertex = Vertex('Color', (2,))\n",
    "edge_list = [\n",
    "    Edge(d_vertex, c_vertex, EdgeType.DIRECTED),\n",
    "    Edge(c_vertex, b_vertex, EdgeType.DIRECTED),\n",
    "    Edge(d_vertex, b_vertex, EdgeType.BI_DIRECTED)\n",
    "]\n",
    "# This will be useful when we pass the input into the generator and for the loss on the labels.\n",
    "vertex_order = [d_vertex, b_vertex, c_vertex]\n",
    "scm = SCM(edge_list).to(DEVICE)\n",
    "# values = scm(x_sample_batch.to(DEVICE))\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    cim = torch.load(MODEL_PATH, weights_only=False, map_location=DEVICE)\n",
    "    logger.info(\"Loading model from checkpoint...\")\n",
    "else:\n",
    "    cim = CausalImageModel(scm)\n",
    "    logger.info(\"Initializing a new model...\")\n",
    "cim.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b9923",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b90a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 19:27:45 - INFO - Number of parameters: 107.01 million\n",
      "2025-04-23 19:27:46 - INFO - Saved the model checkpoint...\n",
      "2025-04-23 19:27:45 - DEBUG - Time it would take for an epoch is -54.536446928977966 minutes with batch size: 8\n",
      "2025-04-23 19:27:45 - DEBUG - Time it would take for an epoch is 11.181026697158813 minutes with batch size: 8\n",
      "2025-04-23 19:27:45 - DEBUG - Time it would take for an epoch is 10.86898148059845 minutes with batch size: 8\n",
      "2025-04-23 19:27:46 - DEBUG - Time it would take for an epoch is 11.022418737411499 minutes with batch size: 8\n",
      "2025-04-23 19:27:46 - DEBUG - Time it would take for an epoch is 11.080056428909302 minutes with batch size: 8\n",
      "2025-04-23 19:27:46 - DEBUG - Time it would take for an epoch is 11.005684733390808 minutes with batch size: 8\n",
      "2025-04-23 19:27:46 - DEBUG - Time it would take for an epoch is 11.024340987205505 minutes with batch size: 8\n",
      "2025-04-23 19:27:46 - DEBUG - Time it would take for an epoch is 11.045873165130615 minutes with batch size: 8\n",
      "2025-04-23 19:27:47 - DEBUG - Time it would take for an epoch is 11.590808629989624 minutes with batch size: 8\n",
      "2025-04-23 19:27:47 - DEBUG - Time it would take for an epoch is 11.056587100028992 minutes with batch size: 8\n",
      "2025-04-23 19:27:47 - DEBUG - Time it would take for an epoch is 11.0902339220047 minutes with batch size: 8\n",
      "2025-04-23 19:27:47 - DEBUG - Time it would take for an epoch is 11.003881692886353 minutes with batch size: 8\n",
      "2025-04-23 19:27:47 - DEBUG - Time it would take for an epoch is 10.853990912437439 minutes with batch size: 8\n",
      "2025-04-23 19:27:47 - DEBUG - Time it would take for an epoch is 10.998323559761047 minutes with batch size: 8\n",
      "2025-04-23 19:27:48 - DEBUG - Time it would take for an epoch is 11.061996221542358 minutes with batch size: 8\n",
      "2025-04-23 19:27:48 - DEBUG - Time it would take for an epoch is 10.873690247535706 minutes with batch size: 8\n",
      "2025-04-23 19:27:48 - DEBUG - Time it would take for an epoch is 10.987013578414917 minutes with batch size: 8\n",
      "2025-04-23 19:27:48 - DEBUG - Time it would take for an epoch is 10.933026671409607 minutes with batch size: 8\n",
      "2025-04-23 19:27:48 - DEBUG - Time it would take for an epoch is 10.933980345726013 minutes with batch size: 8\n",
      "2025-04-23 19:27:49 - DEBUG - Time it would take for an epoch is 10.880634188652039 minutes with batch size: 8\n",
      "2025-04-23 19:27:49 - DEBUG - Time it would take for an epoch is 10.855183005332947 minutes with batch size: 8\n",
      "2025-04-23 19:27:49 - DEBUG - Time it would take for an epoch is 10.941118001937866 minutes with batch size: 8\n",
      "2025-04-23 19:27:49 - DEBUG - Time it would take for an epoch is 11.009380221366882 minutes with batch size: 8\n",
      "2025-04-23 19:27:49 - DEBUG - Time it would take for an epoch is 10.912373661994934 minutes with batch size: 8\n",
      "2025-04-23 19:27:49 - DEBUG - Time it would take for an epoch is 10.882943868637085 minutes with batch size: 8\n",
      "2025-04-23 19:27:50 - DEBUG - Time it would take for an epoch is 11.041447520256042 minutes with batch size: 8\n",
      "2025-04-23 19:27:50 - DEBUG - Time it would take for an epoch is 10.936141014099121 minutes with batch size: 8\n",
      "2025-04-23 19:27:50 - DEBUG - Time it would take for an epoch is 10.933026671409607 minutes with batch size: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Backprop and update the params:\u001b[39;00m\n\u001b[32m     42\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m opt.step()\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Store the loss for later\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Causal-Diffusion-Model/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Causal-Diffusion-Model/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Causal-Diffusion-Model/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "cim.train()\n",
    "\n",
    "num_params = sum(p.numel() for p in cim.parameters())\n",
    "logger.info(f\"Number of parameters: {num_params / 1e6:.2f} million\")\n",
    "\n",
    "# Our loss function\n",
    "loss_fn1 = torch.nn.MSELoss()\n",
    "loss_fn2 = torch.nn.CrossEntropyLoss()\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(cim.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(cim, MODEL_PATH)\n",
    "        logger.info('Saved the model checkpoint...')\n",
    "    start_time = time.time()\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        start_time2 = time.time()\n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(DEVICE)  # Data on the GPU\n",
    "        y_labels = mnist_y_labels(y.to(DEVICE))\n",
    "        noise_amount = torch.randn_like(x).to(DEVICE)  # Pick random noise amounts\n",
    "        timestep = torch.randint(0, 1000, (x.shape[0],)).to(DEVICE) # Pick a random timestep to train on\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise_amount, timestep) # Create our noisy x\n",
    "        # Get the model prediction\n",
    "        pred_noise, gen_factors = cim(x, noisy_x, timestep, vertex_order)  # <<< Using timestep  always, adding .sample\n",
    "        # Calculate the losses for both scm generative factors and the predicted noise.\n",
    "        loss1 = loss_fn1(pred_noise, noise_amount)\n",
    "        loss2 = 0\n",
    "        for logits, labels, in zip(gen_factors, y_labels):\n",
    "            loss2 += loss_fn2(logits, labels)\n",
    "        # Normalize the loss\n",
    "        loss2 /= len(y_labels)\n",
    "        total_loss = loss1 + loss2\n",
    "        \n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        # Store the loss for later\n",
    "        losses.append(total_loss.item())\n",
    "        logger.debug(f\"Time it would take for an epoch is {(time.time() - start_time2) * 30000 / 60 / BATCH_SIZE} minutes with batch size: {BATCH_SIZE}\")\n",
    "    end_time = time.time()\n",
    "    # Print our the average of the loss values for this epoch:\n",
    "    avg_loss = sum(losses[-len(train_loader) :]) / len(train_loader)\n",
    "    logger.info(f\"Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}. Time taken to run epoch: {int((end_time-start_time)/60)} minutes.\")\n",
    "\n",
    "\n",
    "torch.save(cim, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1be813",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Try to perform inference by generating an image using the Causal SCM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a814ff90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDS05La8MUKrBOEcokzRxiO2BR5Au0EHbtQKucZDnsKujTNNhij067tYUkmlEEPnRqElGFL7VX5VLlemc52Z5Kg5MtrPolnFEIY7e2unCSXFlIcNKmSNvzAAlmPXaoK8BQCaqSQWcWorptrNbG2do4ZJZvnEMm7BVcBeApLAkBcqPmBwtbyw9KVTmW7Wm97bbbX3W1/RnXQw1ONJJy0fW2t0+jduui77rdHRXzaXFdW9lHYQXrpH5Yihmji8lWAx95hgsCCFQ5I5GMcxa14g0nTb1YXltYC0YfypmmV1BJ6hBgZ684PPIFLa3EjT6hPDueVFX7a8yeVuKDaCEIzjCMc/L98ZDcFcrW4zBfK8traOZ4lmAu9quoPt2HGfqTUSw6S5Ywu/Jb38rfpru9zkqRlV/d0VJtW2v1W6trr2urqzsW5JLGHTbLR5pVlawtvMlgaz3qSYvMQjLA9xuGQWJ5wDWfNfXNzaJeWn2pIp5/PjCgv++KgMqM2ASrRthQV+XdjIYV1erQra+HbqBSzRC4W3Vc7QFBVsgLj5sknNcvFDbw6fc29rALe3SxW8WJHdkEhhnblXJDDdEp+bPcdDitlUhGPvK8rtdLaat7bt26dLHTRrRpuHlpst9+b/ga9r223v7XE948SzWN2xk3QqJSF8rP/AC0wSoJSRiP7xxjsRBrcs0OoeXLcvb4QbY49S+z4B55UyLk5J5x0wO1UtK1i4kuYrcJCrG6t7czbN0pVtqt8zZOWCAH1yT15rKuL8QFDNZ2t48q799yhZlGSoUEEEgBe+T79KIU+Wba0W2je69LW2/E48J7SnXlSm1JrvdJaJ6NO/wDW2p//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cim.eval()\n",
    "# Create random noise\n",
    "latents = torch.rand((1, IMAGE_CHANNELS, IMAGE_RESOLUTION, IMAGE_RESOLUTION)).to(DEVICE) * noise_scheduler.init_noise_sigma\n",
    "# The original image we set to random during generation\n",
    "I = torch.rand(1, IMAGE_CHANNELS, IMAGE_RESOLUTION, IMAGE_RESOLUTION).to(DEVICE)\n",
    "\n",
    "# Sampling loop\n",
    "noise_scheduler.set_timesteps(1000)  # Adjust steps as needed\n",
    "reset_scm = True\n",
    "for t in noise_scheduler.timesteps:\n",
    "    with torch.no_grad():\n",
    "        noise_pred, gen_factors = cim(I, latents, t, vertex_order, reset_scm=reset_scm)\n",
    "        reset_scm = False # After the first generation, we reuse the scm.\n",
    "    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "# Decode from the latent space into the image space\n",
    "pred_image = latents\n",
    "image_list = [image for image in pred_image]\n",
    "new_image = torch.cat(image_list, dim=1)\n",
    "visualize_image(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974be228",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
